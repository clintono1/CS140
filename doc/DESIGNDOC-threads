			+--------------------+
			| CS 140             |
			| PROJECT 1: THREADS |
			| DESIGN DOCUMENT    |
			+--------------------+
				   
---- GROUP ----

>> Fill in the names and email addresses of your group members.

Song Han <songhan@stanford.edu>
Jinchao Ye <jinchao@stanford.edu>
Bo Wang <bowang@stanford.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

None

>> Describe briefly which parts of the assignment were implemented by
>> each member of your team. If some team members contributed significantly
>> more or less than others (e.g. 2x), indicate that here.

Song Han: alarm, priority scheduler, system merge and debug, design document 
Jinchao Ye: advanced scheduler, design document
Bo Wang: priority donation, design document, testing cases

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

			     ALARM CLOCK
			     ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

---thread.h---
Changes to 'struct thread'
    int64_t wake_up_time;            /* Time to wake up current thread */
    struct list_elem alarm_elem;     /* List element for alarm queue */

---timer.c---
Changes to 'timer.c'
    static struct list alarm_list; /* List of threads waiting for alarm */

---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

When a a thread calls timer_sleep(), we first calculate the wake up time, which
is given by start + ticks. To avoid busy waiting, a thread in sleep should be 
blocked, and a new thread should be scheduled to run. We use alarm_list to keep
track of all sleeping threads. Since this list is globally shared among all 
threads, its modification is protected by disabling interrupts. 
Another reason to disable interrupt is that blocking a thread will incur 
schedule(), so when doing context switch we should disable interrupt.

One thing to consider is the sequence in which the sleeping threads are
inserted. Since at each tick we should check if there's a thread that needs
to be waken up, it's better to sort the queue by the wake up time. 
Thus we implemented and use list_insert_ordered() for thread insertion so that
their ordering is chronological. Thus, the searching complexity is O(1).


>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

We keep a sorted alarm_list at timer_sleep(), so in the timer interrupt 
handler we only need to peek the front of the queue to see if a thread 
needs to be waken up. This complexity is O(k), where k is the number of
threads that should be waken up at the same time.

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

The race condition is avoided by disabling interrupts when we manipulate 
the alarm list and blocking the current thread. Therefore when multiple 
threads are trying to sleep, only one thread can manipulate the alarm_list 
and block it self. This operation is therefore atomic.

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

The timer interrupt handler also manipulates the alarm_list, since it try to
see if the time comes to wake up some thread and get them out of the list. 
However, in both the timer interrupt handler and the timer_sleep, 
manipulation to the global variable of this alarm_list is protected by 
disabling interrupts. So race conditions are avoided. 

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

There are several design considerations in current implementation: first 
the sleeping threads are kept track of through the alarm_list, each thread
is added an additional node, the alarm_elem that link them together. 
Another additional component is the wake_up_time. All these two adds up to 
16 bytes to the structure of thread, since they are small compared to 4KB 
space and they are static allocated, they won't lead to stack over flow. The
other consideration is the sorted alarm_list, which decreased complexity 
during timer interrupt handler. 

One alternative approach, with the motivation of avoid adding more components
in the thread structure, is to make another structure that points to the waking
thread and recording this thread's wake up time at same time. It has an 
additional list_elem component that link them together. 

    struct node
    {
        struct thread *t;     /* pointer to the sleeping thread */
        int64_t wake_up_time; /* when to wake up */
        list_elem element;    /* link these nodes together */
    }

This approach has the advantage of not occupying additional space to thread
structure. However, each time a thread tries to sleep we have to dynamically 
allocate such a node to track the sleeping node, and when it wake up we have 
to free such a node. This overhead should be taken into consideration. However,
in our approach, we don't need to dynamically allocate a space to track the 
thread, we just link the thread itself to the alarm_list. Neither do we need 
to free the space when we wake up the thread. 

			 PRIORITY SCHEDULING
			 ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

---sync.h---
Changes to 'struct lock'
    struct list_elem thread_elem; /* List element for waited lock queue */

---thread.h---
Changes to 'struct thread'
    int eff_priority;                   /* Effective priority */
    struct lock * lock_to_acquire;      /* Lock this thread is waiting for */
    struct list locks_waited_by_others; /* Locks held by this thread but 
                                           also waited by other threads */

---thread.c--- 
Changes to global variable ready_list
    static struct list ready_list[64];  /* 64 ready queues*/


>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

The variable eff_priority denotes the current priority with donation. We use 
this separate variable to avoid overwrite the generic priority of the thread.
When a thread A acquires a lock, if not successful, it will add itself to the 
waiting list at lock->semaphore->waiters. Then thread A will look for the 
lock's holder thread and donate priority if A's priority is higher.

The effective priority update is done by calling thread_set_eff_priority 
function on the holder thread. This function will check whether its 
lock_to_acquire is null. If not null, it will recursively call 
itself on the lock holder it is waiting for.

Case 1: Nested donation
This is the case when a donation triggers a chain of donations. The following
figure illustrates the case when the donation chain involes three threads:
Thread H is trying to acquire lock L1, but L1 is currently held by thread M. 
Therefore, thread H donates its priority to M. At the same time, M is waiting 
for L2, which is currently held by L. Since M has just increased its priority
due to H's donation, it must propagate this donation to L, which triggers 
the priority donation from M to L. In the end, both M and L are boosted to H's
priority.

In our data structure, struct lock * lock_to_acquire inside structure thread
is used for nested donation. When a thread's priority is modified, it triggers
pointer-tracing to find lock_to_acquire, and then trace this lock's holder to 
find the thread that is the potential donee.

               _                       _
              / \                     / \
            +-----+                 +-----+
       |--->| L1  |            |--->| L2  |
     wait   +-----+\         wait   +-----+\
       |          ho\der       |          ho\der
--------             \----------             \--------
| High |              | Medium |              | Low  |
--------              ----------              --------
                      


Case 2: Multiple donation
The second senario is multiple donation, in which multiple priorites are 
donated to a single thread, either via a single lock or via multiple locks.
What we should consider is how to recover thread L's priority when it
releases one of its lock (in this exmaple, L releases L2). A naive solution
is to recover it back to it's primitive priority, however, this is not true.

The correct solution is to traverse all the remaining locks that L hold (the 
list locks_waited_by_others, in this example, L1), for each lock, find its 
waiting threads by traversing lock->semaphore->waiters (in this exmple, 
thread M). Pick the highest effective priority among these waiters as well
as its own primitive priority, and set its effective priority to this value. 
In this example, L's priority should be recovered to M when it release the 
lock L2.



        ----------                              ----------
        | Medium |                              |  High  | 
        ----------                              ----------
            | wait                              wait |
            V                                        V
		    _                                        _
		   / \                                      / \
         +-----+   holder   ---------    holder   +-----+
         | L1  | ---------> |  Low  | <---------- | L2  |
         +-----+            ---------             +-----+



---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

For locks and semaphores, we keep waiting threads in lock->semaphore->waiters
in order, based on the eff_priority. Thus when waking up, the first thread in 
the list is the one with the highest priority.

For condition variables, the waiting thread list in its semaphore->waiters list
is not ordered. Thus we will use list_max to find the one with the highest
priority and then remove it from the list to wake up.

For both cases, if the thread released from the waiting list has higher 
priority than the current running thread, then the current thread yields 
immediately.

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

When the lock to acquire is not available, a priority donation is triggered.
Thread A: the acquiring thread
Thread B: the holder thread
1) Set lock_to_acquire in Thread A
2) Check whether this lock is already on Thread B's locks_waited_by_others list.
   if not, insert this lock to Thread B's locks_waited_by_others list. 
3) Insert Thread A into the lock->semaphore->waiters list based on eff_priority
   from big to small.
4) If A's priority is higher than that of B, set B's eff_priority by calling
   thread_set_eff_priority on B
5) Block Thread A

In thread_set_eff_priority function, nested donation is handled.
When thread_set_eff_priority(struct thread *A, int eff_priority) is called:
1) Set thread A's eff_priority to the new eff_priority
2) If thread A's lock_to_acquire is not NULL, nested donation is triggered

   i)   Since the thread's eff_priority is changed, sort the waiters list of
        lock_to_acquire.

   ii)  Find the thread B who is holding the lock that A is trying to acquire.

   iii) If thread A's new eff_priority is higher than that of B, call
        thread_set_eff_priority on the thread B with the new eff_priority. 
	    (This could potentially trigger next level of nested priority donation,
	    since thread_set_eff_priority is called recursively.)

   vi)  If thread A's new eff_priority is less than that of B, first search 
        for the highest priority among all locks_waited_by_others of B and the
		priority of thread B itself; Then call thread_set_eff_priority on the
		thread B with the eff_priority just found.

In a nutshell, nested donation is handled by recursive calls to function
thread_set_eff_priority.

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

1) Disable interrupt

2) The lock->semaphore->waiters list is not empty since a higher-priority
   thread is waiting, pop the first thread B in the list.
   Since this list is inserted in order and resorted once a thread's 
   priority changes, the first thread is the one with the highest priority.

3) Remove the lock from the current thread A's locks_waited_by_others list

4) If A's eff_priority equals to that of B, it is possible that A's
   eff_priority was donated by B. Thus we need to determine A's new 
   eff_priority.
   i)  Find the new highest priority after the lock is removed from A's 
       locks_waited_by_others list by calling thread_find_max_priority(A)
   ii) Call thread_set_eff_priority on A with the eff_priority just found.

5) Set flag yield_on_return to true if A's new eff_priority is smaller than
   that of B.
6) Unblock thread B.
7) Enable interrupt.
8) Thread A yields if yield_on_return is true.

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

thread_set_priority() sets the priority of a thread and updates the effective
priority as well. The new effective priority is found by comparing the priority
of all locks held by the thread and its generic priority. When the new
effective priority is found, it updates thread->eff_priority.

A potential race can occur if a thread A is switch over right after finding the
new effective priority and before setting it to thread->eff_priority. If at 
that time, another thread B with a higher priority kicks in and waits on a lock
A current holds, then it will update A's eff_priority via priority donation. 
However, after this update, A is switched back, then it updates its own 
eff_priority with the previous value calculated (which will be lower), then the
new effective priority is incorrect.

To avoid this issue, a lock does NOT help. That would be just using the problem 
itself to solve itself. We avoid this race condition by disabling interrupts 
inside thread_set_priority().

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

To solve the nested donation case, we added a level of indirection to find which
thread's priority should also change. This is done through the lock. That is, we
first trace the lock, then trace the holder of the lock. 

One alternative design could be directly link the associate thread to current
threadto prepare for nested donation. However, this approach has no advantage:
first, it also add to each thread a thread pointer, it's won't save space. 
Second, this method is hard to maintain when a lock's owner got changed: we 
have to update the waiter's associative thread to the lock's new holder each 
time the holder changes. Consider these factors, we chose our design which add
a level of indirection during nested donation. 

The second consideration is the same with part1, to maintain the waiter list
sorted when inserting. Since this will facilitate picking the highest priority
thread when lock_release/sema_up/signal

The third consideration has to do with the data structure of ready queue. We 
used 64 ready queues instead of a single ready queue. This data structure will
facilitate part3 advanced scheduler as well. We can easily locate where to
insert a ready thread: simply insert in the ready_list[63- (t->priority)]. It's 
complexity is O(1). We can easily find which thread has the highest priority and
schedule it first, while for the same priority threads we can schedule them 
round robin. When counting the total number of ready threads for part 3, we 
should add up all the lengths of the 64 ready queues. 





			  ADVANCED SCHEDULER
			  ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

---thread.h---
add following members to struct thread: 
int recent_cpu;                     /* CPU time received recently */
int nice;                           /* Nice value of each thread*/
#define NICE_MAX 20                     /* Highest nice */
#define NICE_MIn -20                    /* Lowest nice */

---thread.c---
/* The average number of threads ready to run over the past minute */
static int load_avg;

---fixed-point.h---                  /* Fixed-Point Real Arithmetic */
fixed point arithmetic goes here

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0     0   0   0   63  61  59   A
 4     4   0   0   62  61  59   A
 8     8   0   0   61  61  59   B           Round-Robin, FCFS
12     8   4   0   61  60  59   A
16     12  4   0   60  60  59   B           Round-Robin, FCFS
20     12  8   0   60  59  59   A
24     16  8   0   59  59  59   C           Round-Robin, FCFS
28     16  8   4   59  59  58   B
32     16  12  4   59  58  58   A
36     20  12  4   58  58  58   C           Round-Robin, FCFS

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

Yes. At timer ticks 8, 16, 24 and 36, there is ambiguities choosing 
which thread to run. The scheduler specification doesn't specify which 
thread to run when there are multiple threads with the same highest 
priority in the ready queue. In case of tie, we use round-robin among 
highest priority threads. We choose the thread to run according to the 
First-Come-First-Serve policy. We use 64 ready queues, 1 queue per 
priority. Once there is one or more ready threads with the highest 
priority as the running thread, push current running thread back to 
the end of the highest non-empty ready queue. Then we run the first 
thread in the highest non-emtpy ready queue.

Also, we don't know TIMER_FREQ in the above question. We assume that 
it's larger than 36.

This is exactly what our scheduler does.

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

If we increase the amount of code of scheduling inside the interrupt 
context, the "atomic" state will last longer. Since we cannot 
schedule optimizely during between interrupts. It will result
in less concurrency and therefore lower the performance. We put as 
less code inside interrupt context as we could.


---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

Advantages: we used 64 ready queues instead of only one ready queue. 
If there is only one ready queue, either "inserting a new thread" or 
"looking for next thread to run" requires O(n) time. In our 
implementation, however, we have one ready queue for each priority. 
When inserting a new thread, we push it to the back of the 
corresonding ready queue,i.e. ready_list[63- (t->priority)]. This 
only takes O(1) time. When looking for the next thread to run, we 
need to find the first thread in the highest non-empty ready queue, 
which also only takes O(1) time. We recalculated the priority of every
thread every 4 ticks. Once the priority of a thread changes, We remove
the thread from its current ready queue and push it back to the 
corresponding ready queue.

Disadvantages: In our current implemenatation, we updated priority 
every four ticks for every thread.
However, we observe that both recent_cpu of each thread and load_avg is 
updated once per second unless the thread is running. Moreover, the 
priority only changes if recent_cpu changes. Therefore, we can save a 
lot of computation by doing:
  udpate priority of each thread once per second, immediately after
  we updated recent_cpu that second
  update priority every 4 ticks only for threads who had runned during
  the last 4 ticks

Future Plan: We might be able to implement the above strategy of 
updating priority if we had extra time.


>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

			   SURVEY QUESTIONS
			   ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

>> Any other comments?
